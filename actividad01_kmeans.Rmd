---
title: "Análisis de la estabilidad de los centroides en K-Medias en presencia de correlación"
author: "Juan David Rivera,Sandra Milena Berrio, Gustavo Rendón, Verónica Garzón <br/> Universidad Nacional de Colombia - Sede Medellín <br/> Departamento de Ciencias de la Computación y de la Decisión <br/> Decisiones bajo incertidumbre (Optimización para aprendizaje de máquina)"
date: "Semestre 2021-01"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, cache = TRUE)
library(mvtnorm)
library(MBESS)
library(Matrix)
```

### El algoritmo de K-Means

El algoritmo de [K-Means](https://en.wikipedia.org/wiki/K-means_clustering) es una técnica popular de aprendizaje no supervisado para agrupar observaciones. 

Uno de los retos en la aplicación de los métodos de aprendizaje de máquinas es el manejo de información redundante. Se considera que la información es redundante cuando a partir de unas variables se pueden inferir las otras.

Un ejemplo de redundancia es la correlación alta entre variables. Si dos variables están altamente correlacionadas, conocer lo que pasa con una permite saber lo que pasa con la otra. Este problema también se conoce como colinealidad. 

Por otro lado, la estabilidad de un método de aprendizaje de máquina se puede entender de diferentes maneras. Cambios pequeños en el conjunto de entrenamiento no producen cambios significativos en:
* a) en los parámetros estimados del modelo (estabilidad en los parámetros) o 
* b) en las salidas del modelo (cambio en las predicciones del modelo)

Uno de los retos de la redundancia es que puede afectar la estabilidad de los métodos de aprendizaje de máquina. En particular, en K-Medias la estabilidad se puede establecer como la variabilidad de los centroides finales cada vez que se cambian los centroides iniciales. Cuando cambiar los centroides iniciales no modifica los centroides finales, se puede considerar que el método tiene un comportamiento estable respecto a la inicialización.


### Objetivo
Entender cómo la correlación entre las variables numéricas puede afectar la estabilidad de los centroides en el algoritmo de K-Medias utilizando escenarios de simulación.

### Retos de aprendizaje
* Planteamiento de estudios de simulación
* Refuerzo de los conceptos estadísticos de media, varianza, covarianza y correlación, distribución normal multivariada
* Refuerzo del algoritmo de K-Medias


### Metodología
Se deberá desarrollar un experimento de simulación para analizar la estabilidad del algoritmo de K-Means. Para ello se proponen los siguientes pasos.

1. Simular tres grupos de distribuciones normales bivariadas independientes pero con traslape. Es decir que los miembros de cada grupo son $X\sim N_2(\mu_j,\Sigma_j)$, $j=1,2,3$. A continuación se presenta un ejemplo de dos grupos generados a partir de distribuciones normales bivariadas:

```{r}
M_cor<-matrix(c(1,0,0,1),ncol=2)

M_cov<-cor2cov(M_cor,sd=c(1,1))

M_cov_pd<-as.matrix(nearPD(M_cov)$mat)
n1<-50 # Tamaño de la muestra de la clase 1
n2<-80 # Tamaño de la muestra de la clase 2
n3<-60 # Tamaño de la muestra de la clase 3

mu3<-c(0.5,1.5) # Vector de medias de la clase 1
mu4<-c(-2,2.5) # Vector de medias de la clase 2
mu5<-c(4,1.5) # Vector de medias de la clase 3

set.seed(1)
muestra3<-rmvnorm(n=n1,mean=mu3,sigma=M_cov_pd,method="eigen")
muestra4<-rmvnorm(n=n2,mean=mu4,sigma=M_cov_pd,method="eigen")
muestra5<-rmvnorm(n=n3,mean=mu5,sigma=M_cov_pd,method="eigen")

muestra_nosep<-rbind(muestra3,muestra4,muestra5)
clase<-c(rep(-1,n1),rep(0,n2),rep(1,n3))
muestra_nosep_df<-data.frame(muestra_nosep,clase)



plot(muestra_nosep,
     col=(clase+2),
     pch=(clase+2),
     ylim=c(-2,6),xlim=c(-6,6),
     xaxt="n",yaxt="n",
     xlab=expression(x[1]),
     ylab=expression(x[2]),
     main="Ejemplo de tres grupos que se traslapan",
     las=1,cex=1.5,lwd=2)
grid()
legend("topleft",legend=c("Grupo 1", "Grupo 2","Grupo 3"),
       pch=c(1,2,3),col=c(1,2,3),pt.lwd=2,pt.cex=1.8,bty="n")

```

2. Encontrar los centroides con K-Means fijando el método de inicialización de los centroides. Encuentre los centroides para $n_c$ inicializaciones aleatorias.

Centroides
```{r}

set.seed(1234)
kmeans <- kmeans(muestra_nosep_df[c(1,2)], 3, iter.max = 100, nstart = 10)
kmeans$centers


```

```{r}
library(ggplot2)
muestra_nosep_df$cluster <- kmeans$cluster
ggplot() + geom_point(aes(x = X1, y = X2, color = cluster), data = muestra_nosep_df, size = 2) +
  scale_colour_gradientn(colours=rainbow(4)) +
  geom_point(aes(x = kmeans$centers[, 1], y = kmeans$centers[, 2]), color = 'black', size = 3) + 
  ggtitle('Clusters de Datos con k = 3 / K-Means') + 
  xlab('X1') + ylab('X2')
```


3. Suponga que el $i$-ésimo individuo es de la forma $X_i = [x_1^i \quad x_2^i]^T$. Cree la variable $x_3$ como $x_3^i=x_1^i+\epsilon_i$ con $\epsilon_i$ iid de media cero y varianza constante. ¿Cuál es la varianza de $x_3$? ¿Cuál es la covarianza entre $x_1$ y $x_3$? ¿Al agregar esta variable K-Means sigue detectando correctamente los centroides? ¿Qué pasa con la estabilidad de los centroides cuando la varianza de $\epsilon$ aumenta?

```{r,echo=TRUE}
set.seed(1234)
e <-rnorm(190,0,1)

muestra_nosep_df$X3 <- muestra_nosep_df$X1+e

```
Varianza de $x_3$

```{r}
var(muestra_nosep_df$X3)
```
Covarianza de $x_3$

```{r}
cov(muestra_nosep_df$X1,muestra_nosep_df$X3)

```
Hallando de nuevo los centroides

```{r}

set.seed(1234)
kmeans3 <- kmeans(muestra_nosep_df[c(1,2,5)], 3, iter.max = 100, nstart = 10)
kmeans3$centers

```
#### Estabilidad de los centroides

```{r,echo=TRUE}
var_interG <- function(data){
  matriz_kc <-matrix(0 ,nrow = 10, ncol = 3)
  for ( i in 1:10 ){
    set.seed(1234)
    var <- seq(1,100,10)
    e <-rnorm(190,0,var[i])
    data$X3 <- data$X1+e
    kc <- kmeans( data[c('X1','X2','X3')], 3 ,iter.max = 100, nstart=10)
    matriz_kc[i,] <- kc$withinss
  }
  return(matriz_kc)
}

matriz_kc <- var_interG(muestra_nosep_df)

varianza <- seq(1,100,10)
plot(varianza,matriz_kc[,1], type = "b" , col='green',ylim = c(0,max(matriz_kc)),ylab='Variblidad Intergrupos',xlab=expression(epsilon))
lines(varianza,matriz_kc[,2],type = "b",col="red")
lines(varianza,matriz_kc[,3],type = "b",col="blue")

legend(x = "topleft", legend = c("Grupo1", "Grupo2","Grupo3"), fill = c("green", "red","blue"), 
       title = "Clusters")
grid()

```

De forma iterativa se puede observar que a medida $\epsilon$ aumenta la variabilidad intra-cluster o dentro de los mismos grupos aumenta, un valor pequeño de esta medida es un indicador de alta estabilidad mientras una varianza grande indica baja o poca estabilidad.


# 4. Creamos las variables $X_4$ y $X_5$
Como en el paso anterior, cree las variables $x_4$ y $x_6$ como la suma de $x_2$ y otra variable de media cero y varianza constante y la variable $x_5$ como la suma de $x_3$ y otra variable de media cero y varianza constante. ¿Al agregar estas variables, K-Means sigue detectando correctamente los centroides? ¿Qué pasa cuando la estabilidad de los centroides cuando la varianza de las variables que se suman a las variables originales aumenta?
```{r mas_ruido}

set.seed(123)
e4 <-rnorm(190,0,3)
e5 <-rnorm(190,0,9)

muestra_nosep_df$X4 <- muestra_nosep_df$X2+e4
muestra_nosep_df$X5 <- muestra_nosep_df$X3+e5
head(muestra_nosep_df)
```
```{r}
kmeans4 <- kmeans(muestra_nosep_df[c(1,2,5,6,7)], 3, iter.max = 100, nstart = 10)
```

```{r}
kmeans4$centers
```

```{r}

set.seed(123)
e4 <-rnorm(190,0,40)
e5 <-rnorm(190,0,30)

muestra_nosep_df$X4 <- muestra_nosep_df$X2+e4
muestra_nosep_df$X5 <- muestra_nosep_df$X3+e5

kmeans4_2 <- kmeans(muestra_nosep_df[c(1,2,5,6,7)], 3, iter.max = 100, nstart = 10)
kmeans4_2$centers
```

Los centroides cambian significativamente así la aleatoriedad de la iniciación se mantenga constante. En métodos de aprendizaje no supervisado es relativo decir que los centroides están bien o mal calculados, pero si se puede decir que incrementando la varianza los centroides cambian.


```{r,echo=TRUE}
var_interG <- function(data){
  matriz_kc <-matrix(0 ,nrow = 10, ncol = 3)
  for ( i in 1:10 ){
    set.seed(1234)
    var <- seq(1,100,10)
    e <- rnorm(190,0,var[i])
    e4 <-rnorm(190,0,var[i])
    e5 <-rnorm(190,0,var[i])
    data$X4 <- data$X2+e4
    data$X5 <- data$X3+e5
    data$X3 <- data$X1+e
    kc <- kmeans( data[c('X1','X2','X3','X4')], 3 ,iter.max = 100, nstart=10)
    matriz_kc[i,] <- kc$withinss
  }
  return(matriz_kc)
}

matriz_kc <- var_interG(muestra_nosep_df)

varianza <- seq(1,100,10)
plot(varianza,matriz_kc[,1], type = "b" , col='green',ylim = c(0,max(matriz_kc)),ylab='Variblidad Intergrupos',xlab=expression(epsilon))
lines(varianza,matriz_kc[,2],type = "b",col="red")
lines(varianza,matriz_kc[,3],type = "b",col="blue")

legend(x = "topleft", legend = c("Grupo1", "Grupo2","Grupo3"), fill = c("green", "red","blue"), 
       title = "Clusters")
grid()

```

#### Estabilidad de los centroides

Aunque la variabilidad Inter-grupos si aumenta a medida que $\epsilon$ aumenta, para los tres grupos aumentan de manera homogénea siendo muy parecida o no tan diferente entre los tres grupos.

5. A partir de estos experimentos, ¿qué se podría decir del efecto de la correlación entre variables y la estabilidad de los centroides en K-Medias?


Cuando se entrena un K-Means con variables correlacionadas, la metodología para hallar los centroides óptimos no convergen de manera eficiente tal que reduzca la variabilidad entre los grupos, además de generar un costo computacional mayor hasta encontrar los puntos óptimos de los centroides.


### Equipos y fecha de entrega
1. El trabajo se realizará en equipos de tres a cinco estudiantes.
2. El código generado para el estudio se debe publicar en GitHub.
3. El reporte se debe publicar como una entrada de blog (por ejemplo en GitHub o [Rpubs](https://rpubs.com)). Debe contener:
 + Planteamiento del problema
 + Descripción de la metodología
 + Resultados
 + Bibliografía
4. La entrega se hará a través de la plataforma Google Classroom del curso en el espacio correspondiente para ello.


### Bibliografia

+ https://rpubs.com/Joaquin_AR/310338


